Traceback (most recent call last):
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/client.py", line 1204, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/asyncio/base_events.py", line 646, in run_until_complete
    return future.result()
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/client.py", line 663, in async_execute
    await self.async_execute_cell(
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/client.py", line 965, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/mriki/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/nbclient/client.py", line 862, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# hyper-params
max_epochs = 3
lr = 0.01
batch_size = 10000
L = 10
n_batches = len(target) // batch_size
DEVICE ="cuda:0"

# define models
cbow = CBoW(V, L).to(DEVICE)
optimizer = optim.Adam(cbow.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()

# training loop
monitoring_loss = []
for epoch in trange(max_epochs):
    with tqdm(total=n_batches) as tbar:
        for batch in get_batch(contexts, target, batch_size):
            x,y = batch
            x,y = x.to(DEVICE), y.to(DEVICE)

            optimizer.zero_grad()
            logits = cbow(x)
            loss = criterion(logits, y)
            loss.backward()
            optimizer.step()
            
            monitoring_loss.append(float(loss))
            tbar.update(1)

# plot
plt.plot(monitoring_loss)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mAssertionError[0m                            Traceback (most recent call last)
Cell [0;32mIn[12], line 10[0m
[1;32m      7[0m DEVICE [38;5;241m=[39m[38;5;124m"[39m[38;5;124mcuda:0[39m[38;5;124m"[39m
[1;32m      9[0m [38;5;66;03m# define models[39;00m
[0;32m---> 10[0m cbow [38;5;241m=[39m [43mCBoW[49m[43m([49m[43mV[49m[43m,[49m[43m [49m[43mL[49m[43m)[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mDEVICE[49m[43m)[49m
[1;32m     11[0m optimizer [38;5;241m=[39m optim[38;5;241m.[39mAdam(cbow[38;5;241m.[39mparameters(), lr[38;5;241m=[39mlr)
[1;32m     12[0m criterion [38;5;241m=[39m nn[38;5;241m.[39mCrossEntropyLoss()

File [0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/torch/nn/modules/module.py:1145[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
[1;32m   1141[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1142[0m                     non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[1;32m   1143[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m, non_blocking)
[0;32m-> 1145[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

File [0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/torch/nn/modules/module.py:797[0m, in [0;36mModule._apply[0;34m(self, fn)[0m
[1;32m    795[0m [38;5;28;01mdef[39;00m [38;5;21m_apply[39m([38;5;28mself[39m, fn):
[1;32m    796[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
[0;32m--> 797[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
[1;32m    799[0m     [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
[1;32m    800[0m         [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
[1;32m    801[0m             [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
[1;32m    802[0m             [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
[0;32m   (...)[0m
[1;32m    807[0m             [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
[1;32m    808[0m             [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

File [0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/torch/nn/modules/module.py:820[0m, in [0;36mModule._apply[0;34m(self, fn)[0m
[1;32m    816[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
[1;32m    817[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
[1;32m    818[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
[1;32m    819[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 820[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
[1;32m    821[0m should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
[1;32m    822[0m [38;5;28;01mif[39;00m should_use_set_data:

File [0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/torch/nn/modules/module.py:1143[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
[1;32m   1140[0m [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
[1;32m   1141[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
[1;32m   1142[0m                 non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
[0;32m-> 1143[0m [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m,[49m[43m [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[43m [49m[43mnon_blocking[49m[43m)[49m

File [0;32m~/.pyenv/versions/miniforge3-4.10.3-10/envs/datasci/lib/python3.10/site-packages/torch/cuda/__init__.py:239[0m, in [0;36m_lazy_init[0;34m()[0m
[1;32m    235[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    236[0m         [38;5;124m"[39m[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with [39m[38;5;124m"[39m
[1;32m    237[0m         [38;5;124m"[39m[38;5;124mmultiprocessing, you must use the [39m[38;5;124m'[39m[38;5;124mspawn[39m[38;5;124m'[39m[38;5;124m start method[39m[38;5;124m"[39m)
[1;32m    238[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mhasattr[39m(torch[38;5;241m.[39m_C, [38;5;124m'[39m[38;5;124m_cuda_getDeviceCount[39m[38;5;124m'[39m):
[0;32m--> 239[0m     [38;5;28;01mraise[39;00m [38;5;167;01mAssertionError[39;00m([38;5;124m"[39m[38;5;124mTorch not compiled with CUDA enabled[39m[38;5;124m"[39m)
[1;32m    240[0m [38;5;28;01mif[39;00m _cudart [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    241[0m     [38;5;28;01mraise[39;00m [38;5;167;01mAssertionError[39;00m(
[1;32m    242[0m         [38;5;124m"[39m[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?[39m[38;5;124m"[39m)

[0;31mAssertionError[0m: Torch not compiled with CUDA enabled
AssertionError: Torch not compiled with CUDA enabled

