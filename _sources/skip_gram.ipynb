{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::{note}\n",
    "\n",
    "このノートは [Skip-Gram実装課題](./skipgram.myst.md) のヒントになるように書かれています．\n",
    "\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBoWと遂になる単語埋め込みベクトル作成手法である __Skip-Gram__ を実装します．ここでは，Negative Samplingのような技術を使わず，出力層でsoftmax関数を利用することで実装を簡単にしています．そのため計算コストが膨大になる傾向があり，大規模なコーパスに適用することはお勧めしません．\n",
    "\n",
    "Skip-Gramの計算コストの大きさには出力層のSoftmax活性化関数が大きな影響を与えます．そのため，高速化を行うためにはSoftmaxを __Negative Sampling__ と呼ばれるアルゴリズムで代用することになります．これについては[このブログ](https://rf00.hatenablog.com/entry/2019/03/17/112317)が実装の助けになります．また，直接Skip-Gramを紹介しているわけではないのですが，CBoWの説明の中でこれを説明している[ゼロから作るDeep Learning ❷ ―自然言語処理編](https://www.amazon.co.jp/%E3%82%BC%E3%83%AD%E3%81%8B%E3%82%89%E4%BD%9C%E3%82%8BDeep-Learning-%E2%80%95%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86%E7%B7%A8-%E6%96%8E%E8%97%A4-%E5%BA%B7%E6%AF%85/dp/4873118360)も非常に参考になるでしょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packageのimport\n",
    "import re\n",
    "import math \n",
    "from typing import Any\n",
    "from tqdm.std import trange,tqdm\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "# pytorch関連のimport\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier, NeuralNetRegressor\n",
    "from skorch.callbacks import Callback, EpochScoring\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパスにはja.text8のサブセットを利用します．発展課題に取り組む場合も，指定されているハイパーパラメータやコーパスのサイズが実行困難である場合は適宜修正してください．ただし，その場合はskip_gram.pyの先頭行に，docstringを用意してその旨を書いてください．あるいはCLIのオプションにしてもいいかもしれません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ちょん 掛け （ ちょん がけ 、 丁 斧 掛け ・ 手斧 掛け と も 表記 ） と は 、 相撲 の 決まり 手 の ひとつ で ある 。 自分 の 右 （ 左 ） 足 の 踵 を 相手 の 右 （ 左 ） 足 の 踵 に 掛け 、 後方 に 捻っ て 倒す 技 。 手斧 （ ちょう な ） を かける 仕草 に 似 て いる こと から 、 ちょう な が 訛っ て ちょん 掛け と なっ \n",
      "1000000/ 46507793\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/ja.text8\") as f:\n",
    "    text8 = f.read()\n",
    "print(text8[:200])\n",
    "\n",
    "#LIMIT = math.floor(len(text8)*0.1)\n",
    "LIMIT = 100_0000\n",
    "print(f\"{LIMIT}/ {len(text8)}\")\n",
    "text8 = text8[:LIMIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コーパス内の単語（トークン）全てを利用すると語彙が多くなりすぎるので，ここでは名詞（それも一般名詞と固有名詞）のみを利用します．そのために形態素解析を行う必要があるので，python製の形態素解析器であるjanomeを利用しています．形態素解析器はこれ以外にもMecabなどが有名です．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 形態素解析器 janome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ja.text8の一部に品詞分解を行なった結果を以下に示します．\n",
    "\n",
    "\n",
    "::::{margin}\n",
    ":::{warning}\n",
    "ja.text8は予め分かち書きされているため，以下の処理が正しく動作している保証はありません．\n",
    ":::\n",
    "::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ちょん \t ['名詞', '一般', '*', '*']\n",
      "掛け \t ['名詞', '接尾', '一般', '*']\n",
      "（ \t ['記号', '括弧開', '*', '*']\n",
      "ちょん \t ['名詞', '一般', '*', '*']\n",
      "がけ \t ['名詞', '接尾', '一般', '*']\n",
      "、 \t ['記号', '読点', '*', '*']\n",
      "丁 \t ['名詞', '固有名詞', '人名', '姓']\n",
      "斧 \t ['名詞', '一般', '*', '*']\n",
      "掛け \t ['名詞', '接尾', '一般', '*']\n",
      "・ \t ['記号', '一般', '*', '*']\n",
      "手斧 \t ['名詞', '一般', '*', '*']\n",
      "掛け \t ['名詞', '接尾', '一般', '*']\n",
      "と \t ['助詞', '格助詞', '引用', '*']\n",
      "も \t ['助詞', '係助詞', '*', '*']\n",
      "表記 \t ['名詞', 'サ変接続', '*', '*']\n",
      "） \t ['記号', '括弧閉', '*', '*']\n",
      "と \t ['助詞', '格助詞', '引用', '*']\n",
      "は \t ['助詞', '係助詞', '*', '*']\n",
      "、 \t ['記号', '読点', '*', '*']\n",
      "相撲 \t ['名詞', '一般', '*', '*']\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "sample_text = \"\".join(text8[:50].split())\n",
    "for token in t.tokenize(sample_text):\n",
    "    print(token.surface, \"\\t\", token.part_of_speech.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### janomeを使った語彙辞書作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活用する語彙をまとめた辞書（word2id, id2word）を作成します．この実装はダーティなので，実際に自然言語処理を行う場合は参考にしないでください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "363003it [00:18, 19741.90it/s]\n",
      "100%|██████████| 80264/80264 [00:00<00:00, 816091.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contextsのshape: (80264, 17871)\n"
     ]
    }
   ],
   "source": [
    "def my_analyzer(text):\n",
    "    #text = code_regex.sub('', text)\n",
    "    #tokens = text.split()\n",
    "    #tokens = filter(lambda token: re.search(r'[ぁ-ん]+|[ァ-ヴー]+|[一-龠]+', token), tokens)\n",
    "    tokens = []\n",
    "    for token in tqdm(t.tokenize(text)):\n",
    "        pos = token.part_of_speech.split(\",\")\n",
    "        if \"名詞\" == pos[0]:\n",
    "            if \"一般\" == pos[1] or \"固有名詞\" == pos[1]:\n",
    "                tokens.append(token.surface)\n",
    "    tokens = filter(lambda token: re.search(r'[ぁ-ん]+|[ァ-ヴー]+|[一-龠]+', token), tokens)\n",
    "    return tokens \n",
    "\n",
    "def build_contexts_and_target(corpus, window_size:int=5)->tuple[np.ndarray,np.ndarray]:\n",
    "    contexts = []\n",
    "    target = []\n",
    "    vocab = set()\n",
    "    _window_size = window_size//2\n",
    "    # 文ごとに分割\n",
    "    preprocessed_corpus = corpus.replace(\" \",\"\")\n",
    "    # posを見て単語ごとに分割\n",
    "    tokens = list(my_analyzer(preprocessed_corpus))\n",
    "\n",
    "    # 新しい語彙を追加\n",
    "    vocab = vocab | set(tokens)\n",
    "\n",
    "    # スライディングウィンドウ\n",
    "    for i in trange(_window_size, len(tokens)-_window_size):\n",
    "        # ウィンドウの真ん中をtargetにする\n",
    "        target.append(tokens[i])\n",
    "        # 真ん中以外の単語をcontextsへ\n",
    "        tmp = tokens[i-_window_size:i]\n",
    "        tmp += tokens[i+1:i+1+_window_size]\n",
    "        contexts.append(tmp)\n",
    "\n",
    "    # 辞書作成\n",
    "    id2word = list(vocab)\n",
    "    word2id = {word:id for id,word in enumerate(id2word)}\n",
    "    vocab_size = len(word2id)\n",
    "\n",
    "\n",
    "    # contextsとtargetを単語id配列へ置き換え\n",
    "    contexts_id_list = [[word2id[word] for word in doc] for doc in contexts]\n",
    "    target_id_list = [word2id[word] for word in target]\n",
    "\n",
    "\n",
    "    contexts = lil_matrix((len(contexts_id_list), vocab_size),dtype=np.float32)\n",
    "    for index, _contexts_id_list in enumerate(contexts_id_list):\n",
    "        #tmp = np.eye(vocab_size)[np.array(_contexts_id_list)]\n",
    "        for word_id in _contexts_id_list:\n",
    "            contexts[index, word_id] +=1.\n",
    "\n",
    "    target = np.array(target_id_list)\n",
    "    return contexts.tocsr().astype(np.float32), target, word2id, id2word\n",
    "\n",
    "WINDOW_SIZE = 11\n",
    "contexts, target, word2id, id2word = build_contexts_and_target(text8, window_size=WINDOW_SIZE)\n",
    "print(f\"contextsのshape: {contexts.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-gramをnn.Moduleのサブクラスとして実装します．\n",
    "\n",
    "![](https://cdn-ak.f.st-hatena.com/images/fotolife/r/rf00/20190316/20190316165423.png)\n",
    "\n",
    "クラスの実装には上のskip-gramアーキテクチャ図を参考にしてください．高速化のテクニックなどは不要です．（もちろん実装できる人は実装してもOK）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int)->None:\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, input:torch.Tensor)->torch.Tensor:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数の作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gramはクラス分類の体裁をとっているので，損失関数にはCross Entropyを用います．ただしPyTorchで用意されているnn.CrossEntropyを用いることは（おそらく）できないので，自作しましょう．\n",
    "\n",
    ":::{hint}\n",
    "\n",
    "条件：\n",
    "- batch_size=128, vocab_size=11342のとき，以下が損失関数に入力されると仮定して実装してください．  \n",
    "    - SkipGramがforwardメソッドから出力するtensor．shapeは「torch.Size([128, 11342])」，\n",
    "    - 正解データとして利用するtensor．shapeは「torch.Size([128, 11342])」\n",
    "- callbackにおいて，ここで実装したcross entropyを使ってperplexityを計算します．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BowCrossEntropy(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        inputはSkip-gramの出力です．\n",
    "        targetは予測したいcontextsです．\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainerの準備と訓練\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの実装が終わったら，あとは訓練用のプログラムを書くだけです．この解説ではskorchを利用して楽をします．Skip-Gramはクラス分類の体裁を取っていると言いましたが，出力はcategoricalではなくmultinomialです．つまり __一つのデータに対して正解ラベルが複数あります__ ．これはskorchの`NeuralNetClassifier`では上手く扱えないので，`NeuralNetRegressor` を使っています．\n",
    "\n",
    ":::{note}\n",
    "- NeuralNetClassifierは主に1データ1ラベルの場合に利用します．今回の例でも使えないわけではないのですが，標準で設定された「正答率を表示するコールバック」が動作してしまうので利用を見送りました．\n",
    "- `EpochScoring(lambda net,X=None,y=None: np.exp(net.history_[-1, \"valid_loss\"]), name=\"valid_ppl\"), `はエポックの終わりに呼び出されるコールバック関数の雛形である`EpochScoring`を利用して，Perplexityを計算します．\n",
    "- targetもcontextsもnp.ndarrayのままでfitに渡します．\n",
    "    - trainerが中でdatasetやdataloaderを用意してくれます．\n",
    "    - contextsはscipy.sparse.lil_matrix or scipy.sparse.csr_matrixになっているので，`toarray`メソッドでnp.ndarrayに戻しています．\n",
    ":::\n",
    "\n",
    ":::{margin}\n",
    "\n",
    "今回は実装の簡単さを優先したので，メモリ効率が非常に悪い実装になっていることに注意してください．RAM 16GB程度あれば動作するはずです．\n",
    "\n",
    "contexts配列のshapeが(80264, 17871)であり，dtype=float32である場合，\n",
    "```python\n",
    "import sys \n",
    "K = 1024\n",
    "M = K**2\n",
    "print(f\"csr_matrix: {sys.getsizeof(contexts)} B\")\n",
    "print(f\"np.ndarray: {sys.getsizeof(contexts.toarray())/M} MB\")\n",
    "```\n",
    "contexts変数に紐づくオブジェクトの使用メモリは以下の通り：\n",
    "```\n",
    "csr_matrix: 48 B\n",
    "np.ndarray: 5471.794036865234 MB\n",
    "```\n",
    "通常の配列で保持すると，shape[0]*shape[1]に比例してメモリを消費します．BoWのような要素がほぼほぼ0で一部が0以外の行列を疎行列と呼びますが，疎行列に特化した型であるcsr_matrixやlil_matrixを使うと，0以外の要素の値とインデックスのみを保持する設計になっているのでメモリ消費量が劇的に減ります．\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    train_ppl    valid_loss    valid_ppl      dur\n",
      "-------  ------------  -----------  ------------  -----------  -------\n",
      "      1        \u001b[36m9.5161\u001b[0m   \u001b[32m13577.1923\u001b[0m        \u001b[35m9.5077\u001b[0m   \u001b[31m13463.3985\u001b[0m  10.8592\n",
      "      2        \u001b[36m8.6090\u001b[0m    \u001b[32m5480.6118\u001b[0m        9.5838   14527.7893  9.1960\n",
      "      3        \u001b[36m7.9813\u001b[0m    \u001b[32m2925.5963\u001b[0m        9.7863   17787.6748  9.1649\n",
      "      4        \u001b[36m7.5530\u001b[0m    \u001b[32m1906.4801\u001b[0m        9.9466   20880.6048  9.2059\n",
      "      5        \u001b[36m7.2634\u001b[0m    \u001b[32m1427.0374\u001b[0m       10.1053   24473.2919  9.4628\n",
      "      6        \u001b[36m7.0630\u001b[0m    \u001b[32m1167.9362\u001b[0m       10.2429   28083.5366  10.0126\n",
      "      7        \u001b[36m6.9139\u001b[0m    \u001b[32m1006.1204\u001b[0m       10.3765   32096.6788  10.0011\n",
      "      8        \u001b[36m6.8048\u001b[0m     \u001b[32m902.1609\u001b[0m       10.4951   36139.6945  10.0044\n",
      "      9        \u001b[36m6.7210\u001b[0m     \u001b[32m829.6149\u001b[0m       10.6081   40461.6903  9.7140\n",
      "     10        \u001b[36m6.6579\u001b[0m     \u001b[32m778.9301\u001b[0m       10.7094   44773.3498  9.1142\n",
      "     11        \u001b[36m6.6072\u001b[0m     \u001b[32m740.4165\u001b[0m       10.8048   49255.9870  9.2038\n",
      "     12        \u001b[36m6.5677\u001b[0m     \u001b[32m711.7535\u001b[0m       10.8919   53738.0492  9.6758\n",
      "     13        \u001b[36m6.5341\u001b[0m     \u001b[32m688.2272\u001b[0m       10.9744   58362.2080  10.3863\n",
      "     14        \u001b[36m6.5076\u001b[0m     \u001b[32m670.1832\u001b[0m       11.0526   63107.6751  9.5027\n",
      "     15        \u001b[36m6.4837\u001b[0m     \u001b[32m654.3667\u001b[0m       11.1271   67986.8931  9.2246\n",
      "     16        \u001b[36m6.4653\u001b[0m     \u001b[32m642.4536\u001b[0m       11.2007   73184.0116  9.6479\n",
      "     17        \u001b[36m6.4466\u001b[0m     \u001b[32m630.5566\u001b[0m       11.2701   78441.8744  9.6398\n",
      "     18        \u001b[36m6.4328\u001b[0m     \u001b[32m621.8999\u001b[0m       11.3421   84297.0755  10.1711\n",
      "     19        \u001b[36m6.4173\u001b[0m     \u001b[32m612.3747\u001b[0m       11.4086   90096.0264  9.0241\n",
      "     20        \u001b[36m6.4071\u001b[0m     \u001b[32m606.1195\u001b[0m       11.4798   96739.1717  9.6081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=SkipGram(\n",
       "    (embedding): Embedding(17871, 50, max_norm=1)\n",
       "    (linear): Linear(in_features=50, out_features=17871, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = NeuralNetRegressor(\n",
    "    SkipGram(len(word2id), 50),\n",
    "    optimizer=optim.Adam,\n",
    "    criterion=BowCrossEntropy,\n",
    "    max_epochs=20,\n",
    "    batch_size=128,\n",
    "    lr=0.01,\n",
    "    callbacks=[\n",
    "        EpochScoring(lambda net,X=None,y=None: np.exp(net.history_[-1, \"valid_loss\"]), name=\"valid_ppl\"), \n",
    "        EpochScoring(lambda net,X=None,y=None: np.exp(net.history_[-1, \"train_loss\"]), name=\"train_ppl\", on_train=True,)\n",
    "    ],\n",
    "    device=\"cpu\", # 適宜変更\n",
    ")\n",
    "\n",
    "trainer.fit(target, contexts.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 類似単語検索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cbowと同様に単語埋め込みベクトルを使って，類似単語の検索を行います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ロボット\n",
      "1:ユニバーサル \t0.898608386516571\n",
      "2:ポルト \t0.7893995642662048\n",
      "3:ロボティックス \t0.763614296913147\n",
      "4:テラ \t0.742680013179779\n",
      "5:関節 \t0.7259170413017273\n"
     ]
    }
   ],
   "source": [
    "def get_similar_words(query, word_embeddings, topn=5, word2id=word2id, ):\n",
    "    \"\"\"単語埋め込みベクトルを使って似た単語を検索する\n",
    "\n",
    "    Args:\n",
    "        query (str): 類似単語を検索したい単語\n",
    "        topn (int, optional): 検索結果の表示個数. Defaults to 5.\n",
    "        word2id (dict[str,int], optional): 単語→単語idの辞書. Defaults to word2id.\n",
    "        word_embeddings (np.ndarray, optional): 単語埋め込み行列．必ず(語彙数x埋め込み次元数)の行列であること. Defaults to word_embeddings.\n",
    "    \"\"\"\n",
    "    id=word2id[query]\n",
    "    E = (word_embeddings.T / np.linalg.norm(word_embeddings,ord=2, axis=1)).T # {(V,L).T / (V)}.T = (V,L)\n",
    "    target_vector = E[id]\n",
    "    cossim = E @ target_vector # (V,L)@(L)=(V)\n",
    "    sorted_index = np.argsort(cossim)[::-1][1:topn+1] # 最も似たベクトルは自分自身なので先頭を除外\n",
    "\n",
    "    print(f\">>> {query}\")\n",
    "    _id2word = list(word2id.keys())\n",
    "    for rank, i in enumerate(sorted_index):\n",
    "        print(f\"{rank+1}:{_id2word[i]} \\t{cossim[i]}\")\n",
    "\n",
    "word_embeddings = trainer.module_.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "get_similar_words(\"ロボット\", word_embeddings, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> サッカー\n",
      "1:リーグ \t0.734089195728302\n",
      "2:専業 \t0.7245967388153076\n",
      "3:ヴァンフォーレ \t0.6850863695144653\n",
      "4:選手 \t0.6845436692237854\n",
      "5:アルビレックス \t0.6741206645965576\n",
      ">>> 日本\n",
      "1:ほん \t0.6705817580223083\n",
      "2:米国 \t0.6255179047584534\n",
      "3:王者 \t0.6063108444213867\n",
      "4:社団 \t0.5765134692192078\n",
      "5:蓄音機 \t0.5684884786605835\n",
      ">>> 女王\n",
      "1:ヴィクトリアシリーズ \t0.6750556826591492\n",
      "2:後塵 \t0.649889349937439\n",
      "3:ティアラカップ \t0.641579806804657\n",
      "4:ボウラー \t0.6231715083122253\n",
      "5:シェクター \t0.6060587763786316\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'機械学習'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m get_similar_words(\u001b[39m\"\u001b[39m\u001b[39m日本\u001b[39m\u001b[39m\"\u001b[39m, word_embeddings, )\n\u001b[1;32m      3\u001b[0m get_similar_words(\u001b[39m\"\u001b[39m\u001b[39m女王\u001b[39m\u001b[39m\"\u001b[39m, word_embeddings, )\n\u001b[0;32m----> 4\u001b[0m get_similar_words(\u001b[39m\"\u001b[39;49m\u001b[39m機械学習\u001b[39;49m\u001b[39m\"\u001b[39;49m, word_embeddings, )\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mget_similar_words\u001b[0;34m(query, word_embeddings, topn, word2id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_similar_words\u001b[39m(query, word_embeddings, topn\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, word2id\u001b[39m=\u001b[39mword2id, ):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"単語埋め込みベクトルを使って似た単語を検索する\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m        word_embeddings (np.ndarray, optional): 単語埋め込み行列．必ず(語彙数x埋め込み次元数)の行列であること. Defaults to word_embeddings.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39mword2id[query]\n\u001b[1;32m     11\u001b[0m     E \u001b[39m=\u001b[39m (word_embeddings\u001b[39m.\u001b[39mT \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(word_embeddings,\u001b[39mord\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mT \u001b[39m# {(V,L).T / (V)}.T = (V,L)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     target_vector \u001b[39m=\u001b[39m E[\u001b[39mid\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '機械学習'"
     ]
    }
   ],
   "source": [
    "get_similar_words(\"サッカー\", word_embeddings, )\n",
    "get_similar_words(\"日本\", word_embeddings, )\n",
    "get_similar_words(\"女王\", word_embeddings, )\n",
    "get_similar_words(\"機械学習\", word_embeddings, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回の解説ではja.text8のサブセットを利用しているせいで，この単語埋め込みがカバーしている語彙に「機械学習」は含まれていないようです．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
